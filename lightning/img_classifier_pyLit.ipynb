{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5d43ef-c5cb-4838-9c7b-f4ab8eb4db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.plugins.environments import SLURMEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e7d961-bdf5-4379-a9c4-546fe814b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, datetime, os\n",
    "\n",
    "# Essential packages for training an image classifier in PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d735051-4918-46c2-adb6-03276482c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the seed\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92916b9-2a0c-4a9b-ae32-c036a089990d",
   "metadata": {},
   "source": [
    "# Lightning Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687dd314-e6c0-4b0d-b347-3d35cc1910c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size:int = 64, num_workers:int = 10, data_dir: str = './'):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.num_workers = 4\n",
    "        \n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.train_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "\n",
    "        self.val_transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "        \n",
    "        print('Initing class')\n",
    "    def prepare_data(self):\n",
    "        print('Preparing data')\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'train' or stage is None:\n",
    "            print('setup stage fit')\n",
    "            self.trainset=ImageFolder(root=os.path.join(self.data_dir,'train'),\n",
    "                                                transform=self.train_transform)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'val' or stage is None:\n",
    "            print('setup stage test')\n",
    "            self.valset = ImageFolder(root=os.path.join(self.data_dir,'val'),\n",
    "                                              transform=self.val_transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print('Train loader')\n",
    "        return DataLoader(self.trainset, \n",
    "                                          batch_size=self.batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=self.num_workers,\n",
    "                                          pin_memory=True,\n",
    "                                          drop_last=False)\n",
    "    def val_dataloader(self):\n",
    "        print('Validation loader')\n",
    "        return DataLoader(self.valset, \n",
    "                                             batch_size=self.batch_size,\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=self.num_workers,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8818cca-1e30-4537-b205-2d169552f1cf",
   "metadata": {},
   "source": [
    "# Lightning module for training\n",
    "\n",
    "In PyTorch Lightning, we define pl.LightningModule's (inheriting from Module) that organize our code into 5 main sections:\n",
    "\n",
    "- Initialization (__init__), where we create all necessary parameters/models\n",
    "- Optimizers (configure_optimizers) where we create the optimizers, learning rate scheduler, etc.\n",
    "- Training loop (training_step) where we only have to define the loss calculation for a single batch (the loop of optimizer.zero_grad(), loss.backward() and optimizer.step(), as well as any logging/saving operation, is done in the background)\n",
    "- Validation loop (validation_step) where similarly to the training, we only have to define what should happen per step\n",
    "- Test loop (test_step) which is the same as validation, only on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71126500-2655-4069-9cf8-b557414a3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLASSIFY_lit_module(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams - Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name - Name of the optimizer to use -- SGD\n",
    "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = torchvision.models.resnet50(weights=None,num_classes=200)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.CrossEntropyLoss()        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward function that is run when visualizing the graph\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We choose SGD as our optimizers.\n",
    "        optimizer = optim.SGD(self.parameters(), lr=1e-3)\n",
    "        \n",
    "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                              step_size=30, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.loss_module(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(inputs).argmax(dim=-1)\n",
    "        acc = (labels == outputs).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        self.log(\"val_acc\", acc,on_step=False, on_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcc492-2877-4d10-94fb-a891c660fb3b",
   "metadata": {},
   "source": [
    "# Trainer definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96617ad3-4a42-4929-8da3-42014d001c14",
   "metadata": {},
   "source": [
    "Now that the data pipeline and training scheme is defined, we pass them to the Lightning's execution framework to automate the execution of the training workflow:\n",
    "- Epoch and batch iteration\n",
    "- Calling forward, loss eval, and backward passes\n",
    "- Evaluating cross validation\n",
    "- Saving and loading weights\n",
    "- MultiGPU support\n",
    "- Mixed precision training\n",
    "\n",
    "And more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579efec9-c85f-4f3f-8c32-0ac7db88a7de",
   "metadata": {},
   "source": [
    "### Initialize data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a40a8c-d7c6-49bc-b6fd-3c821ed6281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing class\n"
     ]
    }
   ],
   "source": [
    "data = MYDataModule(batch_size=256,\n",
    "                    num_workers=10,\n",
    "                    data_dir=\"/ibex/ai/reference/CV/tinyimagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b4b639-7530-4380-bd98-0c9fbfb757e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n"
     ]
    }
   ],
   "source": [
    "data.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff4b303-2ba9-4d2d-92d2-86810524f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup stage fit\n",
      "setup stage test\n"
     ]
    }
   ],
   "source": [
    "data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de3c10-834a-45cd-ae03-08a2b0c1be66",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230649bd-a570-4638-86b0-99f07bb8a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CLASSIFY_lit_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bdbfe63-0f9f-46d9-9cc6-7d1a5fef0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHPKT_PATH=os.path.join(os.environ['PWD'],'lightning/chkpt')\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=CHPKT_PATH,\n",
    "                                                   filename='{epoch}-{val_loss:.2f}-{other_metric:.2f}',\n",
    "                                                  save_weights_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  monitor='train_acc')\n",
    "logger = TensorBoardLogger(save_dir=\"logs\",\n",
    "                           sub_dir=None,\n",
    "                           name=None,\n",
    "                           version=None,\n",
    "                           default_hp_metric=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f58a95f9-c873-4b73-bc27-e9e986551e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(max_epochs=2,\n",
    "                     logger=logger,\n",
    "                     callbacks=[checkpoint_callback],\n",
    "                     accelerator=\"auto\", \n",
    "                     devices=1, \n",
    "                     num_nodes=1, \n",
    "                     strategy=None,\n",
    "                     plugins=[SLURMEnvironment(auto_requeue=False)],\n",
    "                     benchmark=False,            \n",
    "                     deterministic=True,\n",
    "                     precision=32,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724900c-9979-476c-9afb-879002504da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | ResNet           | 23.9 M\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "23.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.9 M    Total params\n",
      "191.343   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loader\n",
      "Train loader\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18dac4749694c6782bb95c159b61e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time \n",
    "# Train the model \n",
    "trainer.fit(net, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0cc45-2074-4bbb-b912-7addf8dac750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
