{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Boilerplate code",
   "id": "2cd3ca08c0f88ccf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time, gc\n",
    "\n",
    "# Timing utilities\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ],
   "id": "b1e38b8cf57b062d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch, datetime, os, argparse, re\n",
    "\n",
    "# Business as usual\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ],
   "id": "2767492e0a2126e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.manual_seed(43)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ],
   "id": "f4ddd810a26d5f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import and instantiate tensorboard for monitoring model performance\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "id": "f947bc301e7751be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Additional package\n",
    "Required for DDP implementation"
   ],
   "id": "5ef18ef41e0e4940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Learning rate scheduler for progressively modifying LR w.r.t epochs to improve training\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "id": "4b659e045760b6a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Setting resources and variables for training in a Jupyter notebook.\n",
    "In a python script version of the code, this section should be parsed in as arguments."
   ],
   "id": "e8c910743d230353"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Miscellaneous utility funtions",
   "id": "f2bb6b9f839133a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def accuracy(outputs, labels):\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    return torch.sum(preds == labels).item()"
   ],
   "id": "4009c6c344bd706b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader\n",
    "Add a data management section to load and transform data.\n",
    "Here we manage not only the data location but also how it is loaded into memory.\n",
    "\n",
    "***NOTE***: `shuffle=True` when set in `trainSampler` makes the Dataloading buggy only if PyTorch version is > 1.12. The `if` condition takes care of it."
   ],
   "id": "e02585814d1aa7a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dataloader(gpu,world_size,batch_size,num_workers):\n",
    "\n",
    "    trainSampler_shuffle=True\n",
    "    version=float(re.findall(r'\\d+\\.\\d+', torch.__version__)[0])\n",
    "    if version > 1.12:\n",
    "        print('Setting shuffle=False in trainSampler')\n",
    "        trainSampler_shuffle=False\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "# Prepare training data\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),normalize ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),normalize ])\n",
    "\n",
    "\n",
    "\n",
    "    datadir=os.environ['DATA_DIR']\n",
    "    trainset = torchvision.datasets.ImageFolder(root=os.path.join(datadir,'train'),\n",
    "                                                transform=train_transform)\n",
    "    trainSampler = torch.utils.data.distributed.DistributedSampler(trainset,\n",
    "                                                               num_replicas=world_size,\n",
    "                                                               rank=gpu,\n",
    "                                                               shuffle=trainSampler_shuffle,\n",
    "                                                               drop_last=True)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=num_workers,\n",
    "                                          pin_memory=True,\n",
    "                                          sampler=trainSampler)\n",
    "\n",
    "\n",
    "    valset = torchvision.datasets.ImageFolder(root=os.path.join(datadir,'val'),\n",
    "                                              transform=val_transform)\n",
    "    valSampler = torch.utils.data.distributed.DistributedSampler(valset,\n",
    "                                                                  num_replicas=world_size,\n",
    "                                                                  rank=gpu,\n",
    "                                                                 shuffle=False)\n",
    "    valloader = torch.utils.data.DataLoader(valset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=num_workers,\n",
    "                                             pin_memory=True,\n",
    "                                             sampler=valSampler)\n",
    "    return trainloader,valloader"
   ],
   "id": "1c7d91ce3a9a001d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Choose a Neural Network architecture\n",
    "\n"
   ],
   "id": "f90531685d30db66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pre-training\n",
    "net=torchvision.models.resnet50(weights=None,num_classes=200)\n",
    "# Transfer learning\n",
    "#net=torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)"
   ],
   "id": "e572bdcae1234944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Some additions and modifications are required to your training section. E.g.\n",
    "- Define a function for setting up multiple GPU context (using awareness of the environment)\n",
    "    - Here you can select the backend or the communication library to move data between memory of GPUs\n",
    "- Define a function and add the training steps in it\n",
    "    - Wrap model in DistributedDataParallel class\n",
    "    - The model, loss function and optimizer needs to be offloaded to each device using the corresponding gpu_id\n",
    "    - Figure out which tasks will be done exclusively master process (gpu_id==0)\n",
    "        - e.g. printing, writing tensorboard logs, saving and loading checkpoints etc\n",
    "    - Optionally, collect training accurracy and loss metrics on GPU 0 so it can write to tensorboard logs\n",
    "- Define a function that setups up the training environment and then calls the training\n",
    "\n"
   ],
   "id": "b01abcced9b8d21c"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Uncomment this block when using python script\n",
    "def setup():\n",
    "\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    dist_url = \"env://\" # default\n",
    "\n",
    "    # only works with torch.distributed.launch // torch.run (when running script, overwrite from environment)\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    print (f\"{local_rank} : {world_size} , {rank} \\n\")\n",
    "    dist.init_process_group(\n",
    "            backend=\"nccl\",\n",
    "            init_method=dist_url,\n",
    "            world_size=world_size,\n",
    "            rank=rank)\n"
   ],
   "id": "1a50bd38fb08d81b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Uncomment this block when running in a notbook\n",
    "def setup(rank, world_size):\n",
    "\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(backend=\"nccl\",\n",
    "                            rank=rank,\n",
    "                            world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ],
   "id": "1d503a890b371efd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train (net,world_size,rank,args):\n",
    "\n",
    "    if 'LOCAL_RANK' in os.environ.keys():\n",
    "        gpu_id=int(os.environ['LOCAL_RANK'])\n",
    "    else:\n",
    "        gpu_id=rank\n",
    "\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "\n",
    "    # Instantiate Tensorboard writer on process handler for GPU 0\n",
    "    if rank == 0:\n",
    "        writer = SummaryWriter(\"logs/experiment_%s\" %(os.environ['SLURM_JOBID']))\n",
    "\n",
    "\n",
    "\n",
    "    # Enable AMP\n",
    "    scaler = amp.GradScaler()\n",
    "    net.cuda(gpu_id)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu_id)\n",
    "    optimizer = optim.SGD(net.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "\n",
    "    # [Optional]: Set LR scheduler\n",
    "    scheduler =  StepLR(optimizer,step_size=30, gamma=0.1)\n",
    "\n",
    "    trainloader, valloader = dataloader(gpu_id,world_size,\n",
    "                                        args.batch_size,\n",
    "                                        args.num_workers)\n",
    "    # Wrap model as DDP\n",
    "    net = torch.nn.parallel.DistributedDataParallel(net,device_ids=[gpu_id])\n",
    "    start_timer()\n",
    "    print('Starting training on GPU %d of %d -- ' %(rank,world_size))\n",
    "    for epoch in range(args.epochs):  # loop over the dataset multiple times\n",
    "        train_loss = 0.0\n",
    "        trainloader.sampler.set_epoch(epoch)\n",
    "        net.train()\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].cuda(gpu_id, non_blocking=True), data[1].cuda(gpu_id,non_blocking=True)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            with torch.cuda.amp.autocast(enabled=True,\n",
    "                                         dtype=torch.float32):\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valloader.sampler.set_epoch(epoch)\n",
    "        val_loss = 0.0\n",
    "        net.eval()\n",
    "        for i, data in enumerate(valloader):\n",
    "            inputs, labels = data[0].cuda(gpu_id,non_blocking=True), data[1].cuda(gpu_id,non_blocking=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        # Gather accuracy metric from all training units on GPU 0\n",
    "        # to calculate an average over the size training dataset\n",
    "        train_loss = torch.tensor(train_loss).cuda(gpu_id)\n",
    "        dist.reduce(train_loss,0,dist.ReduceOp.SUM)\n",
    "\n",
    "        val_loss = torch.tensor(val_loss).cuda(gpu_id)\n",
    "        dist.reduce(val_loss,0,dist.ReduceOp.SUM)\n",
    "\n",
    "        # Print from GPU 0\n",
    "        if rank == 0:\n",
    "            train_loss = train_loss.item() / len(trainloader.dataset.targets)\n",
    "\n",
    "            val_loss   = val_loss.item() / len(valloader.dataset.targets)\n",
    "\n",
    "            print(f'[{epoch + 1}] :Loss (train, val):{train_loss:.3f}, {val_loss:.3f}')\n",
    "            writer.add_scalar(\"Loss/train\", train_loss , epoch)\n",
    "            writer.add_scalar(\"Loss/val\", val_loss , epoch)\n",
    "            writer.flush\n",
    "\n",
    "        # Save checkpoint every 10th epoch\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            if rank == 0:\n",
    "                PATH='./model_chkpt_ep%d.pth' %(epoch)\n",
    "                torch.save(net.state_dict(), PATH)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    if rank == 0:\n",
    "        end_timer_and_print('Finished Training')\n",
    "        writer.close()"
   ],
   "id": "479c8884e0465e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(net,args):\n",
    "    world_size = args.gpus\n",
    "    setup(rank, world_size)\n",
    "    train(net,world_size,rank,args)\n",
    "    return True"
   ],
   "id": "844247f9ecd95da9"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Uncomment when using as python script\n",
    "if __name__ == '__main__':\n",
    "    world_size= int(os.environ['WORLD_SIZE'])\n",
    "    rank      = int(os.environ['RANK'])\n",
    "    local_rank= int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num-workers\", default=10,\n",
    "                        help=\"number of dataloaders\", type=int)\n",
    "    parser.add_argument(\"--batch-size\", default=256,\n",
    "                        help=\"mini batch size per GPU\", type=int)\n",
    "    parser.add_argument(\"--epochs\", default=5,\n",
    "                        help=\"total epochs\", type=int)\n",
    "    parser.add_argument(\"--lr\", default=1e-3,\n",
    "                        help=\"Learning rate\",type=float)\n",
    "    parser.add_argument(\"--momentum\", default=0.9,\n",
    "                        help=\"Momentum\", type=float)\n",
    "    parser.add_argument(\"--weight-decay\", default=5e-4,\n",
    "                        help=\"Momentum\", type=float)\n",
    "    parser.add_argument(\"--print-interval\", default=100,\n",
    "                        help=\"Momentum\", type=int)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    setup()\n",
    "    # Pre-training\n",
    "    train(net,world_size,rank,args)\n",
    "    cleanup()"
   ],
   "id": "9aa93042315062ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Comment when using as python script\n",
    "class nb_args():\n",
    "    nodes = 1\n",
    "    gpus=int(os.environ['SLURM_GPUS'])\n",
    "    num_workers = 8\n",
    "    batch_size=64\n",
    "    epochs=2\n",
    "    lr=1e-3\n",
    "    momentum=0.9\n",
    "    weight_decay=5e-4\n",
    "    print_interval=100\n",
    "args=nb_args()\n",
    "os.environ['MASTER_ADDR']='localhost'\n",
    "os.environ['MASTER_PORT']='12355'"
   ],
   "id": "bda1fb6c96c3920e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Comment when using as python script\n",
    "import multiprocess as mp\n",
    "num_processes = args.gpus\n",
    "# NOTE: this is required for the ``fork`` method to work\n",
    "net.share_memory()\n",
    "\n",
    "processes = []\n",
    "for rank in range(num_processes):\n",
    "    p = mp.Process(target=main, args=(net,args))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ],
   "id": "d1e8ea83cab19173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75d0abfdabda6b6d"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
