{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "\n",
    "# Timing utilities\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, datetime, os\n",
    "\n",
    "# Business as usual\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate tensorboard for monitoring model performance\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional package \n",
    "Required for DDP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting resources and variables for training in a Jupyter notebook.\n",
    "In a python script version of the code, this section should be parsed in as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=1\n",
    "gpus=2\n",
    "num_workers=10\n",
    "batch_size=512\n",
    "epochs=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous utility funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = outputs.max(dim=1)\n",
    "    return torch.sum(preds == labels).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Add a data management section to load and transform data.\n",
    "Here we manage not only the data location but also how it is loaded into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def dataloader(gpu,world_size,batch_size,num_workers):\n",
    "# Prepare training data\n",
    "    train_transform = transforms.Compose(\n",
    "    [transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    datadir=os.environ['DATA_DIR']\n",
    "    trainset = torchvision.datasets.ImageFolder(root=os.path.join(datadir,'train'),\n",
    "                                                transform=train_transform)\n",
    "    trainSampler = torch.utils.data.distributed.DistributedSampler(trainset,\n",
    "                                                               num_replicas=world_size,\n",
    "                                                               rank=gpu,\n",
    "                                                               shuffle=True,\n",
    "                                                               drop_last=True)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, \n",
    "                                          num_workers=num_workers,\n",
    "                                          pin_memory=True,\n",
    "                                          sampler=trainSampler)\n",
    "                                         \n",
    "\n",
    "    valset = torchvision.datasets.ImageFolder(root=os.path.join(datadir,'val'),\n",
    "                                              transform=val_transform)\n",
    "    valSampler = torch.utils.data.distributed.DistributedSampler(valset,\n",
    "                                                                  num_replicas=world_size,\n",
    "                                                                  rank=gpu,shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(valset, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=num_workers,\n",
    "                                             pin_memory=True,\n",
    "                                             sampler=valSampler)\n",
    "    return trainloader,valloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a Neural Network architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=torchvision.models.resnet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Some additions and modifications are required to your training section. E.g.\n",
    "- Define a function for setting up multiple GPU context (using awareness of the environment)\n",
    "    - Here you can select the backend or the communication library to move data between memory of GPUs\n",
    "- Define a function and add the training steps in it\n",
    "    - Wrap model in DistributedDataParallel class\n",
    "    - The model, loss function and optimizer needs to be offloaded to each device using the corresponding gpu_id\n",
    "    - Figure out which tasks will be done exclusively master process (gpu_id==0)\n",
    "        - e.g. printing\n",
    "- Define a function that setups up the training environment and then calls the training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train (net,gpus,world_size,rank,epochs,batch_size):\n",
    "    gpu_id=rank\n",
    "    scaler = amp.GradScaler()\n",
    "    net.cuda(gpu_id)\n",
    "    net = nn.SyncBatchNorm.convert_sync_batchnorm(net)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu_id)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    local_rank = gpu_id #int(os.environ['LOCAL_RANK'])\n",
    "    trainloader, valloader = dataloader(gpu_id,world_size,\n",
    "                                        batch_size,\n",
    "                                        num_workers)\n",
    "    # Wrap model as DDP\n",
    "    net = torch.nn.parallel.DistributedDataParallel(net,device_ids=[local_rank],\n",
    "                                                   output_device=None, )\n",
    "    start_timer()\n",
    "    print('Starting training on GPU %d of %d'%(gpu_id,world_size))\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss = 0.0\n",
    "        train_acc  = 0\n",
    "        trainloader.sampler.set_epoch(epoch)\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].cuda(gpu_id, non_blocking=True), data[1].cuda(gpu_id,non_blocking=True)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "            train_acc  += accuracy(outputs,labels)           \n",
    "\n",
    "        valloader.sampler.set_epoch(epoch)\n",
    "        val_loss = 0.0\n",
    "        val_acc  = 0\n",
    "        for i, data in enumerate(valloader):\n",
    "            inputs, labels = data[0].cuda(gpu_id,non_blocking=True), data[1].cuda(gpu_id,non_blocking=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() \n",
    "            val_acc  += accuracy(outputs,labels)\n",
    "\n",
    "        # Gather accuracy metric from all training units on GPU 0  \n",
    "        # to calculate an average over the size training dataset    \n",
    "        train_acc = torch.tensor(train_acc).cuda(gpu_id)\n",
    "        dist.reduce(train_acc,0,dist.ReduceOp.SUM)\n",
    "        val_acc = torch.tensor(val_acc).cuda(gpu_id)\n",
    "        dist.reduce(val_acc,0,dist.ReduceOp.SUM)\n",
    "        \n",
    "        # Print from GPU 0\n",
    "        if gpu_id ==0:\n",
    "            val_loss   = val_loss / len(valloader.dataset.targets)\n",
    "            val_acc    = 100 * (val_acc.item() / len(trainloader.dataset.targets))\n",
    "            train_loss = train_loss / len(trainloader.dataset.targets)\n",
    "            train_acc  = 100 * (train_acc.item() / len(trainloader.dataset.targets))\n",
    "            print(f'[{epoch + 1}] :Loss (train, val):{train_loss:.3f}, {val_loss:.3f}| Accuracy (train,val): {train_acc:.3f}, {val_acc:.3f}')\n",
    "\n",
    "    if gpu_id == 0:\n",
    "        end_timer_and_print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(net,gpus,epochs,batch_size):\n",
    "    world_size = 2\n",
    "    setup(rank, world_size)\n",
    "    train(net,gpus,world_size,rank,epochs,batch_size)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/csgv/machine_learning/2022.11/el7_cudnn8.2_cuda11.2_py3.8_env/machine_learning-module/env/lib/python3.9/site-packages/torch/cuda/memory.py:278: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on GPU 0 of 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/csgv/machine_learning/2022.11/el7_cudnn8.2_cuda11.2_py3.8_env/machine_learning-module/env/lib/python3.9/site-packages/torch/cuda/memory.py:278: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on GPU 1 of 2\n",
      "[1] :Loss (train, val):0.006, 0.005| Accuracy (train,val): 0.656, 0.078\n"
     ]
    }
   ],
   "source": [
    "import multiprocess as mp\n",
    "num_processes = gpus\n",
    "# NOTE: this is required for the ``fork`` method to work\n",
    "net.share_memory()\n",
    "processes = []\n",
    "for rank in range(num_processes):\n",
    "    p = mp.Process(target=main, args=(net,gpus,epochs,batch_size))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
